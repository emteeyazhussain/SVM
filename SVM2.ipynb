{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ad92d5-7c27-42f6-b712-d31f69333bea",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. Relationship between Polynomial Functions and Kernel Functions:**\n",
    "Polynomial functions are mathematical functions that involve powers of a variable, like \\(x^2\\), \\(x^3\\), and so on. In machine learning, polynomial kernel functions are used to implicitly transform the input data into a higher-dimensional space, which can help capture non-linear relationships between features. This is done without explicitly calculating the transformations, thanks to the kernel trick.\n",
    "\n",
    "Kernel functions, in general, serve as similarity measures between data points in a higher-dimensional space. The polynomial kernel is one type of kernel function that computes the similarity as the inner product of the transformed feature vectors.\n",
    "\n",
    "**Q2. Implementing SVM with Polynomial Kernel in Python using Scikit-learn:**\n",
    "Here's how you can implement an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVR model with a polynomial kernel\n",
    "svr = SVR(kernel='poly', degree=3)  # Degree is the degree of the polynomial\n",
    "\n",
    "# Fit the model\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "\n",
    "**Q3. Effect of Epsilon Parameter on the Number of Support Vectors in SVR:**\n",
    "In Support Vector Regression (SVR), the epsilon parameter (\\(\\epsilon\\)) determines the width of the epsilon-insensitive tube around the predicted values. Larger values of epsilon allow more training samples to be within the tube, potentially leading to more support vectors. Smaller values of epsilon result in a narrower tube and may reduce the number of support vectors.\n",
    "\n",
    "**Q4. Impact of Parameters on SVR Performance:**\n",
    "- **Choice of Kernel Function:** Different kernel functions (linear, polynomial, radial basis function, etc.) capture different types of relationships in the data. Choose the kernel that aligns with the underlying pattern.\n",
    "- **C Parameter:** The C parameter controls the trade-off between achieving a low training error and a low testing error (soft margin vs. hard margin). A small C allows more errors in training (more tolerance to misclassifications), while a large C aims for fewer errors (less tolerance). Increase C when you want a more complex decision boundary or trust the data labels.\n",
    "- **Epsilon Parameter:** Epsilon (\\(\\epsilon\\)) controls the width of the tube. A larger \\(\\epsilon\\) allows more training samples to be within the tube, leading to more support vectors. Smaller \\(\\epsilon\\) may result in fewer support vectors. Adjust based on the desired level of tolerance for deviations from the predicted values.\n",
    "- **Gamma Parameter:** For the RBF kernel, gamma controls the shape of the decision boundary. A smaller gamma makes the boundary more spread out, potentially leading to overfitting. A larger gamma makes the boundary more focused, potentially leading to underfitting.\n",
    "\n",
    "Remember, these parameter choices often involve trade-offs and should be tuned using techniques like cross-validation to find the best configuration for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a64f6a-cc5a-4f62-9fb2-4b41d92b477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target labels (y)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and best score from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(**best_params)\n",
    "tuned_svc.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'tuned_svc_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee060ae-4a2c-42a4-b836-d9bb0419efbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
